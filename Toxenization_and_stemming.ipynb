{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038427f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kaira\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\kaira\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\kaira\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kaira\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kaira\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaira\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd3a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kaira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaca8c",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347973ed",
   "metadata": {},
   "source": [
    "Converting Paragraphs (corpus) -> Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15a944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts. Separated they live in Bookmarksgrove right at the coast of the Semantics, a large language ocean. A small river named Duden flows by their place and supplies it with the necessary regelialia. It is a paradisematic country, in which roasted parts of sentences fly into your mouth. Even the all-powerful Pointing has no control about the blind texts it is an almost unorthographic life One day however a small line of blind text by the name of Lorem Ipsum decided to leave for the far World of Grammar. The Big Oxmox advised her not to do so, because there were thousands of bad Commas, wild Question Marks and devious Semikoli, but the Little Blind Text didn’t listen. She packed her seven versalia, put her initial into the belt and made herself on the way. When she reached the first hills of the Italic Mountains, she had a last view back on the skyline of her hometown Bookmarksgrove, the headline of Alphabet Village and the subline of her own road, the Line Lane. Pityful a rethoric question ran over her cheek, then\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "corpus = \"\"\"Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts. Separated they live in Bookmarksgrove right at the coast of the Semantics, a large language ocean. A small river named Duden flows by their place and supplies it with the necessary regelialia. It is a paradisematic country, in which roasted parts of sentences fly into your mouth. Even the all-powerful Pointing has no control about the blind texts it is an almost unorthographic life One day however a small line of blind text by the name of Lorem Ipsum decided to leave for the far World of Grammar. The Big Oxmox advised her not to do so, because there were thousands of bad Commas, wild Question Marks and devious Semikoli, but the Little Blind Text didn’t listen. She packed her seven versalia, put her initial into the belt and made herself on the way. When she reached the first hills of the Italic Mountains, she had a last view back on the skyline of her hometown Bookmarksgrove, the headline of Alphabet Village and the subline of her own road, the Line Lane. Pityful a rethoric question ran over her cheek, then\"\"\"\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05c7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523b37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Far far away, behind the word mountains, far from the countries Vokalia and Consonantia, there live the blind texts.\n",
      "Separated they live in Bookmarksgrove right at the coast of the Semantics, a large language ocean.\n",
      "A small river named Duden flows by their place and supplies it with the necessary regelialia.\n",
      "It is a paradisematic country, in which roasted parts of sentences fly into your mouth.\n",
      "Even the all-powerful Pointing has no control about the blind texts it is an almost unorthographic life One day however a small line of blind text by the name of Lorem Ipsum decided to leave for the far World of Grammar.\n",
      "The Big Oxmox advised her not to do so, because there were thousands of bad Commas, wild Question Marks and devious Semikoli, but the Little Blind Text didn’t listen.\n",
      "She packed her seven versalia, put her initial into the belt and made herself on the way.\n",
      "When she reached the first hills of the Italic Mountains, she had a last view back on the skyline of her hometown Bookmarksgrove, the headline of Alphabet Village and the subline of her own road, the Line Lane.\n",
      "Pityful a rethoric question ran over her cheek, then\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c87eb",
   "metadata": {},
   "source": [
    "Converting Paragraphs -> Words and Sentences -> Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb3dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78458b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts words into tokens\n",
    "tokenized_words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3433316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Far', 'far', 'away', ',', 'behind', 'the', 'word', 'mountains', ',', 'far', 'from', 'the', 'countries', 'Vokalia', 'and', 'Consonantia', ',', 'there', 'live', 'the', 'blind', 'texts', '.']\n",
      "['Separated', 'they', 'live', 'in', 'Bookmarksgrove', 'right', 'at', 'the', 'coast', 'of', 'the', 'Semantics', ',', 'a', 'large', 'language', 'ocean', '.']\n",
      "['A', 'small', 'river', 'named', 'Duden', 'flows', 'by', 'their', 'place', 'and', 'supplies', 'it', 'with', 'the', 'necessary', 'regelialia', '.']\n",
      "['It', 'is', 'a', 'paradisematic', 'country', ',', 'in', 'which', 'roasted', 'parts', 'of', 'sentences', 'fly', 'into', 'your', 'mouth', '.']\n",
      "['Even', 'the', 'all-powerful', 'Pointing', 'has', 'no', 'control', 'about', 'the', 'blind', 'texts', 'it', 'is', 'an', 'almost', 'unorthographic', 'life', 'One', 'day', 'however', 'a', 'small', 'line', 'of', 'blind', 'text', 'by', 'the', 'name', 'of', 'Lorem', 'Ipsum', 'decided', 'to', 'leave', 'for', 'the', 'far', 'World', 'of', 'Grammar', '.']\n",
      "['The', 'Big', 'Oxmox', 'advised', 'her', 'not', 'to', 'do', 'so', ',', 'because', 'there', 'were', 'thousands', 'of', 'bad', 'Commas', ',', 'wild', 'Question', 'Marks', 'and', 'devious', 'Semikoli', ',', 'but', 'the', 'Little', 'Blind', 'Text', 'didn', '’', 't', 'listen', '.']\n",
      "['She', 'packed', 'her', 'seven', 'versalia', ',', 'put', 'her', 'initial', 'into', 'the', 'belt', 'and', 'made', 'herself', 'on', 'the', 'way', '.']\n",
      "['When', 'she', 'reached', 'the', 'first', 'hills', 'of', 'the', 'Italic', 'Mountains', ',', 'she', 'had', 'a', 'last', 'view', 'back', 'on', 'the', 'skyline', 'of', 'her', 'hometown', 'Bookmarksgrove', ',', 'the', 'headline', 'of', 'Alphabet', 'Village', 'and', 'the', 'subline', 'of', 'her', 'own', 'road', ',', 'the', 'Line', 'Lane', '.']\n",
      "['Pityful', 'a', 'rethoric', 'question', 'ran', 'over', 'her', 'cheek', ',', 'then']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d616ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Far', 'far', 'away', ',', 'behind', 'the', 'word', 'mountains', ',', 'far', 'from', 'the', 'countries', 'Vokalia', 'and', 'Consonantia', ',', 'there', 'live', 'the', 'blind', 'texts', '.']\n",
      "['Separated', 'they', 'live', 'in', 'Bookmarksgrove', 'right', 'at', 'the', 'coast', 'of', 'the', 'Semantics', ',', 'a', 'large', 'language', 'ocean', '.']\n",
      "['A', 'small', 'river', 'named', 'Duden', 'flows', 'by', 'their', 'place', 'and', 'supplies', 'it', 'with', 'the', 'necessary', 'regelialia', '.']\n",
      "['It', 'is', 'a', 'paradisematic', 'country', ',', 'in', 'which', 'roasted', 'parts', 'of', 'sentences', 'fly', 'into', 'your', 'mouth', '.']\n",
      "['Even', 'the', 'all', '-', 'powerful', 'Pointing', 'has', 'no', 'control', 'about', 'the', 'blind', 'texts', 'it', 'is', 'an', 'almost', 'unorthographic', 'life', 'One', 'day', 'however', 'a', 'small', 'line', 'of', 'blind', 'text', 'by', 'the', 'name', 'of', 'Lorem', 'Ipsum', 'decided', 'to', 'leave', 'for', 'the', 'far', 'World', 'of', 'Grammar', '.']\n",
      "['The', 'Big', 'Oxmox', 'advised', 'her', 'not', 'to', 'do', 'so', ',', 'because', 'there', 'were', 'thousands', 'of', 'bad', 'Commas', ',', 'wild', 'Question', 'Marks', 'and', 'devious', 'Semikoli', ',', 'but', 'the', 'Little', 'Blind', 'Text', 'didn', '’', 't', 'listen', '.']\n",
      "['She', 'packed', 'her', 'seven', 'versalia', ',', 'put', 'her', 'initial', 'into', 'the', 'belt', 'and', 'made', 'herself', 'on', 'the', 'way', '.']\n",
      "['When', 'she', 'reached', 'the', 'first', 'hills', 'of', 'the', 'Italic', 'Mountains', ',', 'she', 'had', 'a', 'last', 'view', 'back', 'on', 'the', 'skyline', 'of', 'her', 'hometown', 'Bookmarksgrove', ',', 'the', 'headline', 'of', 'Alphabet', 'Village', 'and', 'the', 'subline', 'of', 'her', 'own', 'road', ',', 'the', 'Line', 'Lane', '.']\n",
      "['Pityful', 'a', 'rethoric', 'question', 'ran', 'over', 'her', 'cheek', ',', 'then']\n"
     ]
    }
   ],
   "source": [
    "# In wordpunct_tokenize, Punctuations are treated as a seperate token\n",
    "tokenized_words_punct = wordpunct_tokenize(corpus) \n",
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98eb36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Far', 'far', 'away', ',', 'behind', 'the', 'word', 'mountains', ',', 'far', 'from', 'the', 'countries', 'Vokalia', 'and', 'Consonantia', ',', 'there', 'live', 'the', 'blind', 'texts', '.']\n",
      "['Separated', 'they', 'live', 'in', 'Bookmarksgrove', 'right', 'at', 'the', 'coast', 'of', 'the', 'Semantics', ',', 'a', 'large', 'language', 'ocean', '.']\n",
      "['A', 'small', 'river', 'named', 'Duden', 'flows', 'by', 'their', 'place', 'and', 'supplies', 'it', 'with', 'the', 'necessary', 'regelialia', '.']\n",
      "['It', 'is', 'a', 'paradisematic', 'country', ',', 'in', 'which', 'roasted', 'parts', 'of', 'sentences', 'fly', 'into', 'your', 'mouth', '.']\n",
      "['Even', 'the', 'all-powerful', 'Pointing', 'has', 'no', 'control', 'about', 'the', 'blind', 'texts', 'it', 'is', 'an', 'almost', 'unorthographic', 'life', 'One', 'day', 'however', 'a', 'small', 'line', 'of', 'blind', 'text', 'by', 'the', 'name', 'of', 'Lorem', 'Ipsum', 'decided', 'to', 'leave', 'for', 'the', 'far', 'World', 'of', 'Grammar', '.']\n",
      "['The', 'Big', 'Oxmox', 'advised', 'her', 'not', 'to', 'do', 'so', ',', 'because', 'there', 'were', 'thousands', 'of', 'bad', 'Commas', ',', 'wild', 'Question', 'Marks', 'and', 'devious', 'Semikoli', ',', 'but', 'the', 'Little', 'Blind', 'Text', 'didn’t', 'listen', '.']\n",
      "['She', 'packed', 'her', 'seven', 'versalia', ',', 'put', 'her', 'initial', 'into', 'the', 'belt', 'and', 'made', 'herself', 'on', 'the', 'way', '.']\n",
      "['When', 'she', 'reached', 'the', 'first', 'hills', 'of', 'the', 'Italic', 'Mountains', ',', 'she', 'had', 'a', 'last', 'view', 'back', 'on', 'the', 'skyline', 'of', 'her', 'hometown', 'Bookmarksgrove', ',', 'the', 'headline', 'of', 'Alphabet', 'Village', 'and', 'the', 'subline', 'of', 'her', 'own', 'road', ',', 'the', 'Line', 'Lane', '.']\n",
      "['Pityful', 'a', 'rethoric', 'question', 'ran', 'over', 'her', 'cheek', ',', 'then']\n"
     ]
    }
   ],
   "source": [
    "# Slightly different, the full stop in between sentences will not be treated as a token but the last full stop is treated as a token.\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "for sentence in documents:\n",
    "    print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341667",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039b283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eaten\", \"ate\", \"eats\", \"writing\", \"written\", \"writes\", \"programming\", \"programs\", \"programmed\",\"history\",\"finally\", \"finalized\",\"congratulations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a84ae",
   "metadata": {},
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f7653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0590d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01691029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eaten ----> eaten\n",
      "ate ----> ate\n",
      "eats ----> eat\n",
      "writing ----> write\n",
      "written ----> written\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "programmed ----> program\n",
      "history ----> histori\n",
      "finally ----> final\n",
      "finalized ----> final\n",
      "congratulations ----> congratul\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\" ----> \"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c541d",
   "metadata": {},
   "source": [
    "### Regexp Stemmer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06752ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this class, you can easily apply regular expression stemmer algorithms. All the outputs depend on the Regular expression you have given\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77dc90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8addb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eaten ----> eaten\n",
      "ate ----> ate\n",
      "eats ----> eat\n",
      "writing ----> writ\n",
      "written ----> written\n",
      "writes ----> write\n",
      "programming ----> programm\n",
      "programs ----> program\n",
      "programmed ----> programmed\n",
      "history ----> history\n",
      "finally ----> finally\n",
      "finalized ----> finalized\n",
      "congratulations ----> congratulation\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' ----> '+reg_stemmer.stem(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87526f",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ef77a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better than porter stemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c4db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae22488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eaten ----> eaten\n",
      "ate ----> ate\n",
      "eats ----> eat\n",
      "writing ----> write\n",
      "written ----> written\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "programmed ----> program\n",
      "history ----> histori\n",
      "finally ----> final\n",
      "finalized ----> final\n",
      "congratulations ----> congratul\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' ----> '+snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8537d559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli sportingli\n",
      "fair sport\n"
     ]
    }
   ],
   "source": [
    "# Porter stemmer vs Snowball stemmer\n",
    "print(stemming.stem('fairly'),stemming.stem('sportingly'))\n",
    "print(snowball_stemmer.stem('fairly'),snowball_stemmer.stem('sportingly'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac58de3",
   "metadata": {},
   "source": [
    "We realise that stemming does not provide accurate results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
